{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eda8009-396b-4604-8b58-7955de682e73",
   "metadata": {},
   "source": [
    "### Punto 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "483a5447-5bfa-4684-9fcc-65996c282276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instalación de NlTK\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2f5e34-7826-4d61-b366-b29bd1e5625e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f8d963-01cd-4ab6-b9f9-a6b22599886f",
   "metadata": {},
   "source": [
    "## 1) Corpus\n",
    "\n",
    "Un corpus es una colección de textos que se usa para entrenar y evaluar modelos de procesamiento del lenguaje. NLTK incluye varios corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38cdcbaf-e152-4f2f-8fa8-e6dd65036403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/aliocha/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Cargar el corpus de textos de Gutenberg\n",
    "corpus_texto = gutenberg.words('austen-emma.txt')\n",
    "print(corpus_texto[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1a5c98-5faf-44da-b4b0-73e69fd74339",
   "metadata": {},
   "source": [
    "## 2) Tokenización:\n",
    "Es el proceso de dividir un texto en unidades más pequeñas, como palabras o frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0db9ac2-f246-4d20-9ce9-a0d6311be39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aliocha/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', ',', 'me', 'llamo', 'Ale', 'y', 'estoy', 'aprendiendo', 'NLTK', 'en', 'el', 'IFTS', '18', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "texto = \"Hola, me llamo Ale y estoy aprendiendo NLTK en el IFTS 18.\"\n",
    "tokens = word_tokenize(texto)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6148128e-3174-43d1-97b5-fe7b32c08ac7",
   "metadata": {},
   "source": [
    "## 3a) Stemming\n",
    "Reduce las palabras a su raíz (sin tener en cuenta su significado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9831df2-87fb-4afd-bad2-f6569c214df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['corriendo', 'corrió', 'corr']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "palabras = ['corriendo', 'corrió', 'corre']\n",
    "stems = [stemmer.stem(p) for p in palabras]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47388a4-7152-47b2-8495-966a2ae4ddb6",
   "metadata": {},
   "source": [
    "## 3b) Lematización\n",
    "\n",
    "Reduce las palabras a su forma base o lema (considera el significado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e80c1330-1057-4b6c-a0f1-df4d602e4a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/aliocha/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'run', 'run']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "palabras = ['running', 'ran', 'runs']\n",
    "lemas = [lemmatizer.lemmatize(p, pos='v') for p in palabras]\n",
    "print(lemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3610c-7faa-42f9-92f0-64d92044a8ab",
   "metadata": {},
   "source": [
    "## 4) Stopwords\n",
    "\n",
    "Son palabras comunes que no añaden mucho valor en el análisis (como \"el\", \"la\", \"de\" en español)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c90cdb5-5f07-4afb-a499-b5d62509efa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aliocha/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', ',', 'ejemplo', 'simple', 'remover', 'stopwords']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "texto = \"Hola, este es un ejemplo simple para remover stopwords\"\n",
    "palabras = word_tokenize(texto)\n",
    "\n",
    "filtrado = [w for w in palabras if not w.lower() in stop_words]\n",
    "print(filtrado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766433f6-6de9-4842-bdcf-53581b9ef22e",
   "metadata": {},
   "source": [
    "## 5) Clasificación\n",
    "\n",
    "Es el proceso de asignar una etiqueta a un conjunto de datos basado en un modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdd95688-6d22-41f5-aad9-43f1c797b45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sí\n"
     ]
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "from nltk import classify\n",
    "\n",
    "# Datos de ejemplo\n",
    "entrenamiento = [({'llueve': True, 'nublado': False}, 'No'), \n",
    "                 ({'llueve': False, 'nublado': True}, 'Sí')]\n",
    "\n",
    "# Entrenamos un clasificador Naive Bayes\n",
    "classifier = NaiveBayesClassifier.train(entrenamiento)\n",
    "\n",
    "# Clasificamos un nuevo dato\n",
    "nueva_muestra = {'llueve': False, 'nublado': True}\n",
    "print(classifier.classify(nueva_muestra))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a0788f-75dc-4118-9990-d35a9ae85941",
   "metadata": {},
   "source": [
    "## 6) Análisis de Sentimientos\n",
    "\n",
    "Es una técnica que permite determinar si un texto expresa una opinión positiva, negativa o neutra.from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "texto = \"Este es el mejor curso de ciencia de datos que he tomado\"\n",
    "resultado = sia.polarity_scores(texto)\n",
    "print(resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca2da2c6-25a7-4bbe-9677-1c4cf8d2da28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/aliocha/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "texto = \"Este es el mejor curso de ciencia de datos que he tomado\"\n",
    "resultado = sia.polarity_scores(texto)\n",
    "print(resultado)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11ee934-af36-4b68-b94a-2f696918eec5",
   "metadata": {},
   "source": [
    "### Punto 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8297a3cd-57c6-44e8-b69f-8a4be0cc774c",
   "metadata": {},
   "source": [
    "A-Tokenización de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23fcb472-c4f1-4b7d-976f-901c369307f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenización de palabras: ['Hola', ',', 'este', 'es', 'un', 'ejemplo', 'simple', '.', '¡Bienvenido', 'al', 'mundo', 'de', 'NLTK', '!']\n",
      "Tokenización de oraciones: ['Hola, este es un ejemplo simple.', '¡Bienvenido al mundo de NLTK!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aliocha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "texto = \"Hola, este es un ejemplo simple. ¡Bienvenido al mundo de NLTK!\"\n",
    "\n",
    "# Tokenización por palabras\n",
    "tokens_palabras = word_tokenize(texto)\n",
    "print(\"Tokenización de palabras:\", tokens_palabras)\n",
    "\n",
    "# Tokenización por oraciones\n",
    "tokens_oraciones = sent_tokenize(texto)\n",
    "print(\"Tokenización de oraciones:\", tokens_oraciones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa52982-29a4-4d64-95e6-35104408e7c2",
   "metadata": {},
   "source": [
    "B-Eliminación de Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3479f6ff-4fbf-4829-a346-f72eee39cc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens después de eliminar stopwords: ['Hola', ',', 'ejemplo', 'simple', '.', '¡Bienvenido', 'mundo', 'NLTK', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aliocha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "tokens_filtrados = [word for word in tokens_palabras if word.lower() not in stop_words]\n",
    "print(\"Tokens después de eliminar stopwords:\", tokens_filtrados)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe87eecf-acfb-4c5a-bc7c-f122828145ed",
   "metadata": {},
   "source": [
    "Resultado:\n",
    "\n",
    "Tokens después de eliminar stopwords: ['Hola', ',', 'ejemplo', 'simple', '.', '¡Bienvenido', 'mundo', 'NLTK', '!']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7fda85-499d-47da-bac4-5648465bc6da",
   "metadata": {},
   "source": [
    "C-Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb5390bd-1ddc-42e2-8845-67d39ac5afb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras originales: ['Corriendo', ',', 'correrán', ',', 'corría', ',', 'corre']\n",
      "Después del stemming: ['corriendo', ',', 'correrán', ',', 'corría', ',', 'corr']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aliocha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')  # Para descargar el tokenizador si es necesario\n",
    "\n",
    "# Inicializamos el Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Texto de ejemplo\n",
    "texto = \"Corriendo, correrán, corría, corre\"\n",
    "\n",
    "# Tokenizamos el texto en palabras\n",
    "palabras = word_tokenize(texto)\n",
    "\n",
    "# Aplicamos el stemming a cada palabra\n",
    "stems = [stemmer.stem(palabra) for palabra in palabras]\n",
    "\n",
    "print(\"Palabras originales:\", palabras)\n",
    "print(\"Después del stemming:\", stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd5caa4-6297-4d42-b90f-94cfffbc71d0",
   "metadata": {},
   "source": [
    "### Punto 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c96d1-5af7-46ad-a097-588ce6295e96",
   "metadata": {},
   "source": [
    "Ejemplos del uso de VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24724b6a-501b-4c34-91c6-955aeb1b2099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de sentimientos: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/aliocha/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "oracion_sentimiento = \"Este producto es increíblemente bueno y útil.\"\n",
    "\n",
    "sentimiento = sia.polarity_scores(oracion_sentimiento)\n",
    "print(\"Análisis de sentimientos:\", sentimiento)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f5fab5d-2d12-4d43-b2c4-e64e0b4954b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.625, 'pos': 0.375, 'compound': 0.636}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/aliocha/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "texto = \"Me encanta este producto, es excelente y super útil!\"\n",
    "resultado = sia.polarity_scores(texto)\n",
    "print(resultado)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
